{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.nasnet import NASNetMobile as NASNet\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenetv2 import MobileNetV2\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.python.client import device_lib\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import time \n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import cProfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(0)\n",
    "\n",
    "TRAIN_PATH = '../../data/img_data/small/recognized'\n",
    "# TEST_PATH = '../../data/img_data/recognized'\n",
    "\n",
    "# batch size used for validation and training\n",
    "BATCH_SIZE = 680\n",
    "\n",
    "# Number of passes over the data\n",
    "TRAIN_EPOCHS = 100\n",
    "\n",
    "# Percent of the data used for validation\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# size of our images\n",
    "IMG_SIZE = (256, 256)\n",
    "\n",
    "# determines the number of classes and total # of samples\n",
    "walk_dir = os.walk(TRAIN_PATH)\n",
    "\n",
    "# NUM_CLASSES = len(list(walk_dir)[0][1])\n",
    "NUM_CLASSES = 340\n",
    "NUM_SAMPLES = 34000\n",
    "# for root, dirs, files in walk_dir:\n",
    "#     NUM_SAMPLES += len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 256, 256, 1)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 256, 256, 1)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 256, 256, 1)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 256, 256, 1)  0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mobilenetv2_1.00_256 (Model)    (None, 340)          2692948     lambda_13[0][0]                  \n",
      "                                                                 lambda_14[0][0]                  \n",
      "                                                                 lambda_15[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Logits (Concatenate)            (None, 340)          0           mobilenetv2_1.00_256[1][0]       \n",
      "                                                                 mobilenetv2_1.00_256[2][0]       \n",
      "                                                                 mobilenetv2_1.00_256[3][0]       \n",
      "                                                                 mobilenetv2_1.00_256[4][0]       \n",
      "==================================================================================================\n",
      "Total params: 2,692,948\n",
      "Trainable params: 2,658,836\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "single_threaded_model = MobileNetV2(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1), weights=None, classes=NUM_CLASSES)\n",
    "model = multi_gpu_model(single_threaded_model, gpus=4)\n",
    "model.compile(optimizer=Adam(lr=0.002), loss='categorical_crossentropy',\n",
    "              metrics=[categorical_crossentropy, categorical_accuracy, top_3_accuracy])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make transformers\n",
    "train_datagen = ImageDataGenerator(\n",
    "#         rotation_range=30,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=[0.9, 1],\n",
    "#         horizontal_flip=True,\n",
    "#         fill_mode='nearest',\n",
    "        validation_split=VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36483 images belonging to 340 classes.\n",
      "Found 8960 images belonging to 340 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        interpolation='nearest',\n",
    "        subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        interpolation='nearest',\n",
    "        subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() \n",
    "for i in range(50):\n",
    "    next(train_generator)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5,\n",
    "                      min_delta=0.005, mode='max', cooldown=3, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "850/850 [==============================] - 480s 565ms/step - loss: 5.8663 - categorical_crossentropy: 5.8663 - categorical_accuracy: 0.0062 - top_3_accuracy: 0.0197 - val_loss: 5.8219 - val_categorical_crossentropy: 5.8219 - val_categorical_accuracy: 0.0074 - val_top_3_accuracy: 0.0208\n",
      "Epoch 2/100\n",
      "850/850 [==============================] - 454s 534ms/step - loss: 5.8200 - categorical_crossentropy: 5.8200 - categorical_accuracy: 0.0073 - top_3_accuracy: 0.0204 - val_loss: 5.8173 - val_categorical_crossentropy: 5.8173 - val_categorical_accuracy: 0.0068 - val_top_3_accuracy: 0.0181\n",
      "Epoch 3/100\n",
      "850/850 [==============================] - 456s 537ms/step - loss: 5.8161 - categorical_crossentropy: 5.8161 - categorical_accuracy: 0.0070 - top_3_accuracy: 0.0196 - val_loss: 5.8128 - val_categorical_crossentropy: 5.8128 - val_categorical_accuracy: 0.0081 - val_top_3_accuracy: 0.0223\n",
      "Epoch 4/100\n",
      "850/850 [==============================] - 453s 533ms/step - loss: 5.8126 - categorical_crossentropy: 5.8126 - categorical_accuracy: 0.0070 - top_3_accuracy: 0.0202 - val_loss: 5.8095 - val_categorical_crossentropy: 5.8095 - val_categorical_accuracy: 0.0065 - val_top_3_accuracy: 0.0208\n",
      "Epoch 5/100\n",
      "850/850 [==============================] - 456s 536ms/step - loss: 5.8086 - categorical_crossentropy: 5.8086 - categorical_accuracy: 0.0066 - top_3_accuracy: 0.0199 - val_loss: 5.8058 - val_categorical_crossentropy: 5.8058 - val_categorical_accuracy: 0.0078 - val_top_3_accuracy: 0.0218\n",
      "Epoch 6/100\n",
      "  3/850 [..............................] - ETA: 7:15 - loss: 5.8168 - categorical_crossentropy: 5.8168 - categorical_accuracy: 0.0000e+00 - top_3_accuracy: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=((1-VAL_SPLIT)*NUM_SAMPLES)//BATCH_SIZE,\n",
    "            epochs=TRAIN_EPOCHS,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=(VAL_SPLIT*NUM_SAMPLES)//BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
