{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.nasnet import NASNetMobile as NASNet\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.mobilenetv2 import MobileNetV2\n",
    "\n",
    "\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy, categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.python.client import device_lib\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "from keras.backend.tensorflow_backend import set_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_learning_phase(0)\n",
    "\n",
    "\n",
    "TRAIN_PATH = '../../data/img_data/small/recognized'\n",
    "# TEST_PATH = '../../data/img_data/recognized'\n",
    "\n",
    "# batch size used for validation and training\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Number of passes over the data\n",
    "TRAIN_EPOCHS = 100\n",
    "\n",
    "# Percent of the data used for validation\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "# size of our images\n",
    "IMG_SIZE = (256, 256)\n",
    "\n",
    "# determines the number of classes and total # of samples\n",
    "walk_dir = os.walk(TRAIN_PATH)\n",
    "\n",
    "# NUM_CLASSES = len(list(walk_dir)[0][1])\n",
    "NUM_CLASSES = 340\n",
    "NUM_SAMPLES = 34000\n",
    "# for root, dirs, files in walk_dir:\n",
    "#     NUM_SAMPLES += len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3_accuracy(y_true, y_pred):\n",
    "    return top_k_categorical_accuracy(y_true, y_pred, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 256, 256, 1)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 256, 256, 1)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 256, 256, 1)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 256, 256, 1)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mobilenetv2_1.00_256 (Model)    (None, 340)          2692948     lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "                                                                 lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Logits (Concatenate)            (None, 340)          0           mobilenetv2_1.00_256[1][0]       \n",
      "                                                                 mobilenetv2_1.00_256[2][0]       \n",
      "                                                                 mobilenetv2_1.00_256[3][0]       \n",
      "                                                                 mobilenetv2_1.00_256[4][0]       \n",
      "==================================================================================================\n",
      "Total params: 2,692,948\n",
      "Trainable params: 2,658,836\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "single_threaded_model = MobileNetV2(input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1), weights=None, classes=NUM_CLASSES)\n",
    "model = multi_gpu_model(single_threaded_model, gpus=4)\n",
    "model.compile(optimizer=Adam(lr=0.002), loss='categorical_crossentropy',\n",
    "              metrics=[categorical_crossentropy, categorical_accuracy, top_3_accuracy])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make transformers\n",
    "train_datagen = ImageDataGenerator(\n",
    "#         rotation_range=30,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=[0.9, 1],\n",
    "#         horizontal_flip=True,\n",
    "#         fill_mode='nearest',\n",
    "        validation_split=VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 36483 images belonging to 340 classes.\n",
      "Found 8960 images belonging to 340 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        interpolation='nearest',\n",
    "        subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_PATH,\n",
    "        target_size=IMG_SIZE,\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        interpolation='nearest',\n",
    "        subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=0.5, patience=5,\n",
    "                      min_delta=0.005, mode='max', cooldown=3, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "425/425 [==============================] - 386s 908ms/step - loss: 5.8494 - categorical_crossentropy: 5.8494 - categorical_accuracy: 0.0064 - top_3_accuracy: 0.0192 - val_loss: 5.8133 - val_categorical_crossentropy: 5.8133 - val_categorical_accuracy: 0.0077 - val_top_3_accuracy: 0.0208\n",
      "Epoch 2/100\n",
      "425/425 [==============================] - 355s 836ms/step - loss: 5.8119 - categorical_crossentropy: 5.8119 - categorical_accuracy: 0.0067 - top_3_accuracy: 0.0195 - val_loss: 5.8070 - val_categorical_crossentropy: 5.8070 - val_categorical_accuracy: 0.0060 - val_top_3_accuracy: 0.0206\n",
      "Epoch 3/100\n",
      "425/425 [==============================] - 355s 835ms/step - loss: 5.8057 - categorical_crossentropy: 5.8057 - categorical_accuracy: 0.0065 - top_3_accuracy: 0.0205 - val_loss: 5.8024 - val_categorical_crossentropy: 5.8024 - val_categorical_accuracy: 0.0063 - val_top_3_accuracy: 0.0208\n",
      "Epoch 4/100\n",
      "425/425 [==============================] - 356s 837ms/step - loss: 5.8035 - categorical_crossentropy: 5.8035 - categorical_accuracy: 0.0069 - top_3_accuracy: 0.0204 - val_loss: 5.7985 - val_categorical_crossentropy: 5.7985 - val_categorical_accuracy: 0.0069 - val_top_3_accuracy: 0.0200\n",
      "Epoch 5/100\n",
      "425/425 [==============================] - 354s 834ms/step - loss: 5.8010 - categorical_crossentropy: 5.8010 - categorical_accuracy: 0.0073 - top_3_accuracy: 0.0207 - val_loss: 5.7999 - val_categorical_crossentropy: 5.7999 - val_categorical_accuracy: 0.0069 - val_top_3_accuracy: 0.0199\n",
      "Epoch 6/100\n",
      "425/425 [==============================] - 354s 833ms/step - loss: 5.8024 - categorical_crossentropy: 5.8024 - categorical_accuracy: 0.0068 - top_3_accuracy: 0.0207 - val_loss: 5.8021 - val_categorical_crossentropy: 5.8021 - val_categorical_accuracy: 0.0071 - val_top_3_accuracy: 0.0193\n",
      "Epoch 7/100\n",
      "425/425 [==============================] - 353s 831ms/step - loss: 5.8028 - categorical_crossentropy: 5.8028 - categorical_accuracy: 0.0065 - top_3_accuracy: 0.0184 - val_loss: 5.7945 - val_categorical_crossentropy: 5.7945 - val_categorical_accuracy: 0.0074 - val_top_3_accuracy: 0.0217\n",
      "Epoch 8/100\n",
      "137/425 [========>.....................] - ETA: 3:50 - loss: 5.7996 - categorical_crossentropy: 5.7996 - categorical_accuracy: 0.0065 - top_3_accuracy: 0.0192"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=((1-VAL_SPLIT)*NUM_SAMPLES)//BATCH_SIZE,\n",
    "            epochs=TRAIN_EPOCHS,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=(VAL_SPLIT*NUM_SAMPLES)//BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
